{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"W5 ML tf-idf & Kfold.ipynb","provenance":[],"authorship_tag":"ABX9TyNvV6Zweihok6h8tXwhwywE"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Elxs99g7gGkO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1593168501991,"user_tz":-540,"elapsed":10779,"user":{"displayName":"한승희","photoUrl":"","userId":"10235006633134353411"}},"outputId":"9f756525-3e4c-4db9-cd87-73a43adacaef"},"source":["! curl http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron1.tar.gz --output enron1.tar.gz"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 1760k  100 1760k    0     0   326k      0  0:00:05  0:00:05 --:--:--  404k\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tWB1YgaZgzNa","colab_type":"code","colab":{}},"source":["! tar -xf enron1.tar.gz enron1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KUvIbXHsg4SW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593168510152,"user_tz":-540,"elapsed":1545,"user":{"displayName":"한승희","photoUrl":"","userId":"10235006633134353411"}},"outputId":"be928cb3-5143-47b3-f0ac-bb3059e852e3"},"source":["import glob, os\n","\n","emails, labels = [], []\n","parition = 0\n","\n","file_path = 'enron1/spam/'\n","for fname in glob.glob(os.path.join(file_path, '*.txt')):\n","  with open(fname,'r', encoding='ISO-8859-1') as f:\n","    emails.append(f.read())\n","    labels.append(1)\n","\n","file_path = 'enron1/ham/'\n","for fname in glob.glob(os.path.join(file_path, '*.txt')):\n","  with open(fname,'r', encoding='ISO-8859-1') as f:\n","    emails.append(f.read())\n","    labels.append(0)\n","\n","print(f'number of emails = {len(emails)}\\nnumber of labels = {len(labels)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["number of emails = 5172\n","number of labels = 5172\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pv10MH6XhNN1","colab_type":"code","colab":{}},"source":["def letters_only(word):\n","  return word.isalpha()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bz3CyQEnhSPw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1593168570206,"user_tz":-540,"elapsed":2849,"user":{"displayName":"한승희","photoUrl":"","userId":"10235006633134353411"}},"outputId":"54af77e6-0339-4fc5-e31c-39277c50cf22"},"source":["import nltk\n","nltk.download('names')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package names to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/names.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"CVZ1CeMkhVtR","colab_type":"code","colab":{}},"source":["from nltk.corpus import names\n","all_names = set(names.words())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Xp3mUWuhqGW","colab_type":"code","colab":{}},"source":["from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kTn5_Haihsub","colab_type":"code","colab":{}},"source":["def clean_text(doc):\n","  cleaned_doc = []\n","  for word in doc.split(' '):   # split doc. by black(' ')\n","    word = word.lower()   # ABD -> abd\n","\n","    if letters_only(word) and word not in all_names and len(word) > 2:  # remove number and punc. and name entity\n","      cleaned_doc.append(lemmatizer.lemmatize(word))\n","  return ' '.join(cleaned_doc)\n","\n","cleaned_emails = [clean_text(doc) for doc in emails]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wt7uNTMghxPD","colab_type":"code","colab":{}},"source":["import numpy as np\n","from sklearn.model_selection import StratifiedKFold\n","k = 10\n","k_fold = StratifiedKFold(n_splits = k) # StratifiedKFold: shuffle samples and split sample for n folds.\n","\n","# convert to numpy array for more efficient slicing\n","cleaned_emails_np = np.array(cleaned_emails)\n","labels_np = np.array(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gdbq32R1imDj","colab_type":"code","colab":{}},"source":["max_features_option = [2000, 4000, 8000]\n","smoothing_factor_option = [0.5, 1.0, 1.5, 2.0]\n","fit_prior_option = [True, False]\n","auc_record = {}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j1v7G_DLi_dr","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import roc_auc_score\n","\n","for train_indices, test_indices in k_fold.split(cleaned_emails, labels):\n","  X_train, X_test = cleaned_emails_np[train_indices], cleaned_emails_np[test_indices]\n","  Y_train, Y_test = labels_np[train_indices], labels_np[test_indices]\n","  for max_features in max_features_option:\n","    if max_features not in auc_record:\n","      auc_record[max_features] = {}\n","    tv = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, stop_words = \"english\", max_features = max_features)\n","    term_docs_train = tv.fit_transform(X_train)\n","    term_docs_test = tv.transform(X_test)\n","    for smoothing_factor in smoothing_factor_option:\n","      if smoothing_factor not in auc_record[max_features]:\n","        auc_record[max_features][smoothing_factor] = {}\n","      for fit_prior in fit_prior_option:\n","        clf = MultinomialNB(alpha=smoothing_factor, fit_prior=fit_prior)\n","        clf.fit(term_docs_train, Y_train)\n","        prediction_prob = clf.predict_proba(term_docs_test)\n","        pos_prob = prediction_prob[:,1]\n","        auc = roc_auc_score(Y_test, pos_prob)\n","        auc_record[max_features][smoothing_factor][fit_prior] = auc + auc_record[max_features][smoothing_factor].get(fit_prior, 0.0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8gjnrIGl5qh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":461},"executionInfo":{"status":"ok","timestamp":1592399916451,"user_tz":-540,"elapsed":793,"user":{"displayName":"한승희","photoUrl":"","userId":"10235006633134353411"}},"outputId":"65ac0a85-e295-47a4-e89d-f58c7850f82f"},"source":["print('max features / smoothing / fit prior / auc')\n","for max_features, max_features_record in auc_record.items():\n","  for smoothing, smoothing_record in max_features_record.items():\n","    for fit_prior, auc in smoothing_record.items():\n","      print('{:10}{:13}{:12}    {:.4f}'.format(max_features, smoothing, fit_prior, auc/k))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["max features / smoothing / fit prior / auc\n","      2000          0.5           1    0.9821\n","      2000          0.5           0    0.9821\n","      2000          1.0           1    0.9808\n","      2000          1.0           0    0.9808\n","      2000          1.5           1    0.9802\n","      2000          1.5           0    0.9802\n","      2000          2.0           1    0.9799\n","      2000          2.0           0    0.9799\n","      4000          0.5           1    0.9873\n","      4000          0.5           0    0.9873\n","      4000          1.0           1    0.9867\n","      4000          1.0           0    0.9867\n","      4000          1.5           1    0.9867\n","      4000          1.5           0    0.9867\n","      4000          2.0           1    0.9870\n","      4000          2.0           0    0.9870\n","      8000          0.5           1    0.9921\n","      8000          0.5           0    0.9921\n","      8000          1.0           1    0.9925\n","      8000          1.0           0    0.9925\n","      8000          1.5           1    0.9929\n","      8000          1.5           0    0.9929\n","      8000          2.0           1    0.9933\n","      8000          2.0           0    0.9933\n"],"name":"stdout"}]}]}